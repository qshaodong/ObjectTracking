\documentclass[12pt]{article} %Basic document type
\usepackage{times} %Font 
\usepackage{amsmath} %For matrices
\usepackage{amssymb}
\usepackage[section]{placeins} %Allows FloatBarrier command
\usepackage[utf8]{inputenc} %Font encoding
\usepackage[margin=1.25in]{geometry} %Adjust margins
\usepackage{graphicx} %Allows picture import
\usepackage{pdfpages} %Including pdf files 
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{appendix}
\usepackage{mcode}
\graphicspath{{images/}} %Allows picture impor
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{A Real-Time Object Tracking System}
\fancyhead[R]{ECSE 456}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1.5pt}
\newcommand{\HRule}[1][\medskipamount]{\par
  \vspace*{\dimexpr-\parskip-\baselineskip+#1}
  \noindent\rule{\linewidth}{0.2mm}\par
  \vspace*{\dimexpr-\parskip-.5\baselineskip+#1}}
\begin{document}
\begin{titlepage}
\begin{center}
\textsc{\huge McGill University}\\[1.5cm]
\textsc{\LARGE Department of Electrical \& Computer Engineering}\\[1.5cm]
\textsc{\Large ECSE 456 - Final Report}\\[3cm]
\HRule
{\huge \bfseries A Real-Time Object Tracking System \\[.3cm] }
\HRule 
\vspace{1.5cm}
\noindent
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{\Large Authors:}\\
\vspace{.2cm}
Benjamin \textsc{Brown} \\
\textit{benjamin.brown2@mail.mcgill.ca} \\
260450182 \\
\vspace{.2cm}
Taylor \textsc{Dotsikas} \\
\textit{taylor.dotsikas@mail.mcgill.ca} \\
260457719
\end{flushleft}
\begin{flushleft} \large
\emph{\Large Supervisors:}\\
\vspace{.2cm}
Warren \textsc{Gross, Prof.}\\
\vspace{.2cm}
Arash \textsc{Ardakani} 
\end{flushleft}
\end{minipage}%
\end{center}
\end{titlepage}
\pagebreak
\section*{Abstract}
Video processing represents an extremely relevant challenge as both the demand for intelligent, aware systems and the quality of modern video technology increases. This increase in quality comes at a cost of large amounts of data being handled under strict time constraints \cite{3}. This project aimed to study how a common video processing algorithm such as object motion tracking can be accelerated using custom hardware. This report concludes on the background, design, and findings of Phase 1 of the project; where platform research, algorithm research, and software implementation was performed. It was clear that an FPGA would be the best solution for fast, low power hardware implementation and a Kalman filter based algorithm was the best solution for a predictive algorithm not prone to noise. Finally, the large amount of time the software needed to process relatively short videos proved that direct hardware implementation of the algorithm is essential for applications. Software implementation was performed in MATLAB, and hardware implementation will be done on an Altera Cyclone II FPGA during Phase 2 of the project.
\section*{Acknowledgments}
The authors would like to thank Professor Warren Gross, for undertaking the responsibility of supervising this project. We would also like to thank Arash Ardakani for all of his advice, help, and time spent meeting with us throughout the semester. Finally, we would like to thank anyone who contributed to the wealth of online resources about object tracking that we hope to contribute to.
\pagebreak
\tableofcontents
\pagebreak
\section{Abbreviations \& Notation}
\begin{itemize}
\item[] FPGA - Field Programmable Gate Array
\item[] ASIC - Application Specific Integrated Circuit
\item[] CPU - Central Processing Unit
\item[] DFG - Delta Frame Generation
\item[] VGA - Video Graphics Array
\item[] I/O - Input/Output
\end{itemize}
\section{Introduction}
\subsection{Motivation}
Scene recreation and analysis is imperative in digital systems that must understand and react to events in their environment. Some typical examples of this include surveillance, robotics, and human-computer interaction. A variety of sensors can be employed for such a task including ultrasonic, radar, and passive infrared, but all of these sensors do not come close to modeling an environment as completely as a video camera. With the increase in image quality and device accessibility, the video camera seems like the obvious solution. \\\\
However, due to the vast amount of data and system imposed processing time constraints, video processing is a challenge. For instance, the transition to a high-definition video platform produces six times more data than the previous standard-definition one \cite{3}. This project aims to study how a complex video processing algorithm such as real-time object tracking can be greatly accelerated when implemented directly in hardware. Object tracking represents an excellent example of the preceding challenges because it requires capturing an image of a scene, processing the image to locate the object in motion, and reconstructing said scene with emphasis placed on the motion, all in real-time.
\subsection{Applications}
The device generates object localization data that can be used by other systems. Systems that require responsive and accurate data for real time use can are the main applications. These secondary systems can now react to moving objects in the selected environment. \\\\
Specifically our algorithm tracks a single object from a camera that watches a stationary environment. This makes our design ideal for tracking a disturbance in the field of view. For example, the addition of our device to a camera surveillance system would provide data about where and when disturbances take place. This information could be used to quickly react to disturbances such as a trespassers on private property.\\\\
Dedicated hardware allows for simpler connectivity to secondary systems. It presents a modular design which is ideal for testing purposes and debugging. If object tracking needs to be performed on higher definition video, dedicated hardware has its own memory which can be specifically allocated for handling larger image frames. Performance depends on the technical specifications of the hardware being used. Our current hardware is limited due to our budget, however it would be possible to upgrade the hardware to match the needs of the desired application. \\\\
The Kalman filter in our design tracks the object more smoothly and can also guess the location of the object if it disappears from the view temporarily. This further enhances the performance of secondary systems by providing them with more accurate and useful object tracking data, which is the main goal of the device.
\section{Background}
\subsection{Video Processing}
The most fundamental way of understanding video data is to consider it as a collection (static) or stream (real-time) of discrete images called frames. A frame is an $MxN$ matrix of pixels. Each pixel is the smallest, discrete element of the image, and store intensity data about the image. There are many different ways of representing intensity in a pixel. In the case of a color image, the pixel contains multiple values that describe the color space. Common representations of the color space are RGB or YCbCr \cite{1}. In both of these models, each pixel has 3 values.  Since MATLAB's \texttt{VideoReader} class uses RGB, for convenience, this color space is used for the remainder of the project. A single color RGB frame is mathematically described as an $MxNx3$ matrix with the form
\begin{equation}
F_i = 
\begin{bmatrix}
(R_{11}, G_{11}, B_{11}) & ... & (R_{1N}, G_{1N}, B_{1N}) \\
 & &\\
... & ... & ... \\
 & &\\
(R_{M1}, G_{M1}, B_{M1}) & ... & (R_{MN}, G_{MN}, B_{MN}) 
\end{bmatrix}
.
\end{equation}
Where
\[
i = 1, 2, ... , n.
\]
\[
0 \leq R_{ij}, G_{i,j}, B_{i,j} \leq 256
\]
The first frame, $F_1$, is defined as the \textit{base frame}, and the remaining frames are defined as the \textit{current frame} for processing. If a video is $t$ seconds long, the \textit{frame rate }is defined as
\begin{equation}
f = \frac{t}{n}.
\label{eq:framerate}
\end{equation}
In the case of a black and white image, the pixel contains a single value that represents the grayscale intensity. Similar to $F_i$, the grayscale frame also contains $n$ frames, and its elements, $Y_{ij}$, are limited between 0 and 256. However, it is an $MXN$ matrix only. It will become apparent later that conversion between RGB and grayscale is imperative for many video processing algorithms. Unsurprisingly, there are multiple ways to do this. The colorimetric conversion principle converts RGB to grayscale using the following weighted sum:
\begin{equation}
Y_{ij} = .2126 \cdot R_{ij} + .7152 \cdot G_{i,j} + .0722 \cdot B_{i,j}.
\end{equation}
Finally it should be mentioned that video data can be streamed in either progressive or interlaced format. Progressive format is the standard one frame at a time while interlaced divides the frames in half into fields. Each field contains either odd or even rows of it's corresponding frame. While this does requiring processing more frames, it gives a clearer, smoother picture as there are less scene changes between frames \cite{1}. This design choice plays a larger role in Phase 2 of the project, since software implementation uses static (pre-recorded) videos.
\subsection{Optical Flow}
Optical flow addresses the idea of determining apparent motion based on changes in image intensity (i.e. brightness) over space and time \cite{4}. All of the proposed differential and matching (feature-based) techniques proposed in Trucco \& Verri \cite{4} go beyond the scope of this project. The simplest of these algorithms involve derivatives of the brightness constancy equation and a least squares solution at each pixel. Not only would they be difficult to implement in both software and hardware, but they produce what is known as the motion field of the image. This is more information than what is needed for the Kalman filter algorithm that is discussed next, which requires simply knowing the $(x,y)$ coordinate of the object in each frame. \\\\
A far simpler, less robust approach is used of locating the object in motion for each frame known as Delta Frame Generation (DFG) \cite{8}. This method makes the following assumptions about the image:
\begin{enumerate}
\item The object of interest is in motion.
\item The object of interest is the only part of the scene in motion.
\item The first frame of the sequence does not contain the object.
\item The lighting and background of the scene does not change between frames.
\end{enumerate}
The delta frame is then calculated as 
\begin{equation}
\Delta_i = | F_i - F_1 |.
\end{equation}
Where $F_i$ is the current grayscale frame and $F_1$ is the base grayscale frame discussed in the previous section. Assuming the above assumptions hold, the delta frame, $\Delta$, will be non-zero for only the object. Knowing that $\Delta$ contains only the object, drawing a line from the top/bottom most points, and finding its intersection with another line from left/right most points produces the center point of the object. The mathematical background for this is as follows. By means of simple search algorithms, the top, bottom, left, and right most points are located as
\[
p_t = (x_1, y_1) ,
\]
\[
p_b = (x_2, y_2) ,
\]
\[
p_l = (x_3, y_3) ,
\]
\[
p_r = (x_4, y_4) .
\]
Then the two lines are given as 
\[
l_1(t,b): y = m_1 x + b_1,
\]
\[
l_2(l,r): y = m_2 x + b_2.
\]
The intersection of these two lines, denoted $(x^*, y^*)$ is given as
\[
x^* = \frac{b_1 - b_2}{m_2 - m_1} = \frac{((x_1y_2) - (y_1x_2))(x_3 - x_4) - (x_1 - x_2)((x_3y_4) - (y_3x_4))}{D},
\]
\[
y^*  = \frac{m_2 b_1 - m_1 b_2}{m_2 - m_1} = \frac{((x_1y_2) - (y_1x_2))(y_3 - y_4) - (y_1 - y_2)((x_3y_4) - (y_3x_4)}{D}.
\]
The common denominator in both results is denoted $D$, and can be expanded as
\[
D = (x_1 - x_2)(y_3 - y_4) - (y_1 - y_2)(x_1 - x_2).
\]
When $D = 0$, the case of parallel lines occurs. In this case, the middle of the object is simply taken to be the average of the top and bottom points.
\subsection{Kalman Filter}
A Kalman filter is used to combine continuous predictions from a theoretical system model with continuous measurements from a real implementation of the same system. This is done to help reduce the effects of noise on system measurements, and can provide a prediction of the system's next state if for some reason the measurements fail or contain lots of noise. Implementation of a Kalman filter is highly intuitive for a sensor that has been characterized with testing. But for object tracking, it is much more difficult to understand as all of the Kalman filter jargon is focused around "sensors" and "measurements" that sound odd when referring to an object tracking system. \\\\
The Kalman filter equations can be divided into two key sets: the predication equations and the update equations \cite{4}, \cite{12}.\\\\
\textbf{Prediction Equations:}
\begin{equation}
\vec{x_{k+1}} = F \cdot \vec{x_k} + B \cdot \vec{u_k}
\label{eq:firstkalman}
\end{equation}
\begin{equation}
P_{k+1} = F \cdot P_k \cdot F^T + Q
\end{equation}
\textbf{Intermediate Calculations:}
\begin{equation}
\vec{y_{k+1}} = \vec{z_k} - H \cdot \vec{x_{k+1}}
\end{equation}
\begin{equation}
S_{k+1} = H \cdot P_{k+1} \cdot H^T + R
\end{equation}
\begin{equation}
K_{k+1} = P_{k+1} \cdot H^T \cdot S_{k+1}^{-1}
\end{equation}
\textbf{Update Equations:}
\begin{equation}
\hat{\vec{x_{k+1}}} = \vec{x_{k+1}} + K_{k+1} \cdot \vec{y_{k+1}}
\end{equation}
\begin{equation}
\hat{P_{k+1}} = (I - K_{k+1} \cdot H) \cdot P_{k+1}
\end{equation}
There are a few key observations that can be immediately made about this set of equations. Note that the distinction between intermediate calculations and predication equations is one that is not usually made in descriptions of Kalman filtering like the one by Trucco \& Verri [4]. However, from an implementation perspective this distinction makes sense, as the system in which the Kalman filter is placed in (or main algorithm) is blind to these equations. Also notice that matrices and vectors without subscripts are ones that do not change each iteration, and remain constant from initialization to completion. \\\\
While all elements in this set of equation have physical meanings and distinct characteristics, for the sake of simplicity the only once that need to be discussed in detail are the inputs, outputs, and constant variables. The two quantities already discussed are the measurement vector $\vec{z}$ of length $m$ and the state vector $\vec{x}$ of length $n$. For this application, $m = 2$ since by methods of optical flow discussed previously we are able to get a 2D Cartesian coordinate for the object.
\begin{equation}
\vec{z} = \begin{bmatrix}
x \\
y \\
\end{bmatrix}
\end{equation}
The length of the state vector depends on the model that has been chosen for the system. Two common models for object tracking applications are the constant velocity model and the constant acceleration model. The assumptions made in these models is self explanatory; $n=4$ for constant velocity and $n=6$ for constant acceleration. In this system, the constant velocity model is used.
\begin{equation}
 \vec{x} = \begin{bmatrix}
x_p \\
y_p \\
v_x \\
v_y
\end{bmatrix}
\end{equation}
The next system dependent variable is the state transition matrix $F$ which is $nxn$. As equation ~\ref{eq:firstkalman} indicates, it describes how the theoretical, predicted behavior of the system changes with each iteration \cite{12}. For a 2D constant velocity model, this matrix is just implementing kinematic equations:
\begin{equation}
x_{new} = x_{old} + t * v_x
\end{equation}
With a similar equation existing for $y$. In matrix form this is 
\begin{equation}
F= \begin{bmatrix}
1 & 0 & f & 0 \\
0 & 1 & 0 & f \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}.
\end{equation}
Where $f$ is the frame rate (i.e. the time step) described in equation ~\ref{eq:framerate}. Also stemming from equation ~\ref{eq:firstkalman}, the vector $\vec{u}$ describes any external inputs applied to the system between iterations. For a simple, constant velocity object tracking system, there are none. This makes the vector irrelevant as well the associated matrix $B$. Getting back to the measurements, the only constant matrix that incorporates $m$ is the measurement model $H$ \cite{4}. This is an $mxn$ matrix that relates the predicated state to the measured value. For this application, it should take the vector $x$ of length 4 and place it in the same vector space as $z$ of length 2. Thus,
\begin{equation}
H = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 
\end{bmatrix}.
\end{equation}
The final two constant matrices that must be initialized prior to starting are $Q$ ($nxn$) and $R$ ($mxm$). These matrices are related to the error in the theoretical model (i.e. air resistance, friction) and the error in the measurement model (i.e. your "sensors", in this case optical flow) respectively \cite{12}. In theory, if there are vectors $\vec{w}$ and $\vec{v}$ that characterize the error in the model and measurement respectively, then $Q$ and $R$ should be diagonal matrices containing the variance of these vectors \cite{4}. \\\\
The final, and perhaps most important, value that should be touched on is the co-variance matrix $P$ ($nxn$) which changes with each iteration. In simple terms, this matrix is a measure of how well the measurements follow the model, and thus how accurate the filter is \cite{12}. Unlike the previously discussed values, $P$ will eventually converge to certain values do to the iterative nature of the algorithm.
\subsection{Fixed vs. Floating-Point}
Fractional portions of numbers can be represented in either fixed or floating-point in hardware. The essential difference between these two data representations is that in fixed-point the decimal point (i.e. dividing the integer and fractional portions) does not really exist from a hardware perspective. It is the job of the programmer to "know" where the decimal point is for each binary string, and then use the data under that interpretation. Most modern computing solutions use floating-point numbers, as it allows the user to implement real numbers without constraint. For example, by default all numbers in MATLAB are 64-bit double precision \cite{10}. A 64-bit architecture (i.e. all numbers are 64-bits long) as well as a floating-point unit take up considerable space in hardware \cite{10}, and embedded hardware solutions have limited resources and power. It is clear that floating-point data representation will not be possible for an application such as this, and fixed-point conversion of the software is necessary. \\\\
Fixed-point conversion works by scaling all real (non-integer) numbers by a large number and then rounding to make it an integer. The idea is that if the number is scaled large enough, rounding will only remove a very small portion of the number, if any. The scale factor is generally used as a power of 2 or power of 10. Powers of 10 are much more intuitive from a programming sense, as humans use base-10, but powers of 2 are much more useful for hardware implementation, as multiplication by a power of 2 represents a shift along a binary string. The object tracking algorithm discussed in the previous sections only uses addition, subtraction, multiplication, and division operations. There is one matrix inverse operation, but since it is a $2x2$ matrix it is treated as swapping entries and dividing. When treating division operations (there are only two) as multiplication by an inverse and subtraction as addition by a negative number, the algorithm can be converted from floating-point to fixed-point by implementing the algorithms for conversion, addition, and multiplication discussed in \cite{10}. \\\\
MATLAB implementation of the three algorithms just mentioned are shown below. Note that every fixed-point number has an associated word length $W$ and a fractional portion $F$. $W$ denotes how long the binary string used to represent the number is, and $F$ denotes the location of the decimal point from the least-significant (rightmost) bit. \\\\
\textbf{Fixed-Point Conversion:}
\begin{lstlisting}
function [fixed] = floatToFix(float, F)

    d = float .* 2^(F);
    fixed = round(d);

    %Verify that enough fractional bits were used to represent this number
    if (float(1,1) ~= 0 && fixed(1,1) == 0)
        disp('ERROR: The chosen F is too small to represent this value.');
    end

end
\end{lstlisting}
\textbf{Fixed-Point Addition:}
\begin{lstlisting}
function [result, F] = fixedAdd(fix1, F1, fix2, F2)

	%Normalize
    if(F1 > F2)
        fix2 = fix2.* 2^(F1-F2);
        F = F1;
    else
        fix1 = fix1.* 2^(F2-F1);
        F = F2;
    end

    result = fix1 + fix2;

end
\end{lstlisting}
\textbf{Fixed-Point Multiplication:}
\begin{lstlisting}
function [result, F] = fixedMult(fix1, F1, fix2, F2)

    result = fix1*fix2;
    F = F1 + F2;

end
\end{lstlisting}
Going through the floating-point software and replacing all non-integer numbers with their converted value as well as replacing all operations with these algorithms converted the software to fixed-point. Finally, it is important to note that using these algorithms recursively or repeatedly causes the fractional portion $F$ to grow significantly. Due to architecture constraints described above, it is necessary to normalize the fixed-point numbers by "chopping" extra bits when there are too many. This is done in MATLAB by calling \texttt{floatToFix.m} with a negative fractional component, and would be done in VHDL or other hardware description languages by simply removing bits since they are accessible binary strings.
\section{Requirements}
Several requirements had to be met so that the device could be viable for use. We placed a strong emphasis on code efficiency in our software development. Computational time dedicated to video processing needs to be kept at a minimum so that the object tracking is done as close to real time as possible. The main goal is to have a live video feed from a camera displayed on a screen and a cursor tracking the object. This will be visual proof that the device works. If our code was inefficient or too intensive, there would be a delay between what was actually happening in front of the camera, and the output video feed that has the object located. The device would not have much use if there was a lot of lag. There will be some delay as it would be impossible to accomplish the necessary computations instantly, but it is possible to minimize the delay so that it is barely noticeable. This is why it was crucial to make sure computations are not redone if they don't need to be and unnecessary computations are removed. Every computation needs to be done in the optimal way considering we don't have accesses to more advanced hardware. These optimization challenges will be more apparent when we begin hardware implementation with VHDL. The hardware limitations of the FPGA certainly pose some constraints to our design but we should be able to obtain a good result with it. \\\\
All other algorithm based requirements have been met at this point in development. The ability to filter out noise and locate a single object has been implemented in our MATLAB code. 
\section{Design}
\subsection{Platform}
During the beginning of the project, rapid research was performed in order to determine what the best embedded hardware solution would be to implement the object tracking algorithm on. This design choice needed to incorporate findings from similar projects in order to avoid common failures, as well as factors specific to this project. \\\\
The main embedded hardware solutions seen today for applications that required dedicated computing are microprocessors, ASICs, and FPGAs. Prior to weighing the best solutions, it is useful to identity the challenges that come with video processing in real-time. Challenges that are paramount in embedded video processing are algorithm complexity, timing requirements, and the need for high processing power (fast clock cycles) \cite{2}.  An FPGA surpasses microprocessors and ASICs in a number of ways for video processing  solutions \cite{1}, \cite{2}:
\begin{itemize}
\item Full control over the internal workings of the FPGA allows for a parallel architecture unlike microprocessors.
\item Configuration after completion allows for bug fixes and hardware modifications.
\item Internal logic of microprocessors is not custom, and thus not application specific.
\item ASIC development cost is extremely high.
\item Video signals have moved to digital formats (some exceptions such as VGA), making programmable logic solutions more relevant.
\end{itemize}
In addition to these findings, an Altera White Paper \cite{3} on video processing describes the ideal architecture video processing hardware should have as high performance, flexible, easy to upgrade, and low priced to develop. From these past projects and studies it is clear that the best hardware solution for algorithm implementation is an FPGA. \\
The next key topic for research was choosing the specific FPGA manufacturer to use. Choosing the right manufacturer is important because the quality of the tools, resources, and support they provide will have a major impact on the ease of development. Starting with the research of similar projects, Roth \cite{1} chose to use an FPGA from the Altera Cyclone family for a real-time video application over Lattice of Xilinx due to the fact that Lattice had poor online support an Altera was lower cost than Xilinx. It was also found that Altera Quartus II was superior to Xilinx ISE Design Suite because it encapsulated the optimization and configuration process into one piece of software (i.e. Quartus II can perfrom compile, simulation, and programming) \cite{1}. In a similar project, Saeed et al. \cite{2} took advantage of the Altera DE2 breakout board by using it's on-board composite video input port as well as the video decoder module. \\\\
Based on these two projects, it was very convincing that Altera should be the manufacturer of choice. However as mentioned before, it is equally important to account for the requirements of this project specifically when choosing the platform for development. Independent of \cite{1}, \cite{2}, and \cite{3}, an Altera breakout board such as the DE1 or DE2 seemed to be the best solution for multiple reasons. \\\\
\textbf{Familiarity:} Both members of this project have experience using Altera Quartus II with VHDL on a Cyclone FPGA. Using familiar hardware and development tools will make implementation in Phase 2 easier. \\\\
\textbf{Avoiding Fabrication:} Due to the constraints on time and money in this project, it will be best to avoid having to fabricate a custom breakout board for the chosen FPGA and any necessary I/O or peripherals. Due to this, the major manufacturers such as Altera, Xilinx, and Lattice should be assessed on their breakout boards, and not just their FPGAs.\\\\
\textbf{Cost:} This project has no funding, and the hardware platform will not be expanded past a single working prototype for this project. Due to this, a low-cost (ideally free), educational development board is the ideal solution. McGill has Altera DE1 and DE2 boards that may be available for use during Phase 2 of this project. If not, the price of the Altera DE1 board is only \$127 for academic use (\$150 commercial) and the DE2 board is only \$284 for academic (\$495 commercial) as well. Investigating comparable solutions by Xilinx for video and image processing applications found the Spartan-6 FPGA Embedded Kit which costs \$695.
\\\\
\textbf{Peripherals:} In order to give the system real-time video capabilities, the breakout board would ideally have some hardware for video decoding and display generation. This is not a necessity, as the live video input feed could be streamed digitally from a computer using  a serial connection like USB. The live video output feed could be streamed back to the computer to be displayed. The scenario just described would use the FPGA only for processing, making the full system require a CPU. However, if the entire system was to entirely autonomous from a laptop of other CPU, the breakout board would need to be able to handle incoming video data (analog or digital) and then drive a display. Examining the schematics of the Altera DE1 and DE2 boards in Figures ~\ref{fig:de1} and ~\ref{fig:de2} located in Appendix ~\ref{sec:add} shows that the DE2 is capable of being a fully functional system because of the 10-bit high speed video DAC (ADV7123) between the FPGA and VGA output port while the DE1 board only has a 4-bit RGB block that is speculated to not provide the needed output picture. It should also be noted that the DE2 board has a video decoder block (ADV7181) placed between the composite video (analog) input port and the FPGA while the DE1 does not.\\\\ 
In summary, it was decided based on the research of similar projects, as well as taking into account certain project specific parameters like cost, time, and familiarity that an Altera DE1 or DE2 board would be the best FPGA breakout board solution. The DE2 board comes equipped with peripherals and I/O that are speculated to make it a fully featured system if analog video input is used,. However a fully featured system not a requirement, as the goal of this project is to implement just the algorithm in hardware.
\subsection{Algorithm}
Discuss the merits of the algorithms we researched and why we chose DFG with a Kalman filter.
\subsection{Software}
Discuss how the software works.
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{./images/software_flow.jpg}
\caption{Software Flowchart}
\label{fig:sw_flow}    
\end{figure}
\section{Future Work}
Ben \& Taylor
\section{Impact on Society}
Our dedicated hardware object tracking system will have some impacts in society, but no profound negative impacts. The most notable impact will be its automating effect in the applications it is integrated with. Its introduction to another system will fulfill a task that could have been previously accomplished by a human or even by software previously embedded within the system. The consumer would have to purchase our hardware to implement our system. Our choice of hardware, the Altera DE2 Board has an upfront cost of around \textdollar500. This can be a substantial price for the consumer but the performance benefits outweigh that of embedded software, and in the case of surveillance for example, the cost of running our system will be less than paying a worker to watch a video feed for objects. The introduction of new technology that makes a manned job obsolete is often viewed as a negative impact on society, but this sort of progress is inevitable. \\\\
In the design process so far, we have not made any sort of environmental impact as we have just been developing software. Our system is going to be a combination of previously manufactured parts which were presumably made with the environment in mind. There are so many electronics already in use in society today, the addition of our system's environmental impact can not be quantified. Our product does not produce waste nor does it need constant physical additions. It requires a small constant supply of electricity. When it is no longer needed it can be properly recycled in the same way that computers and other electronics are. Our system does not pose any health or safety risks.\\\\
As mentioned earlier, the value of our device comes from the additional performance gained by a secondary system using it. It is possible that the secondary system has some sort of malicious intent but our product cannot be responsible for any negative consequences the secondary system produces. 
\section{Allocation of Work}
Ben \& Taylor
\section{Conclusion}
Ben \& Taylor
\newpage
\begin{thebibliography}{11}
\bibitem{1}
F. Roth, “Using low cost FPGAs for realtime video processing”, M.S. thesis, Faculty of Informatics, Masaryk University, 2011.
\bibitem{2}
A. Saeed et al., “FPGA based Real-time Target Tracking on a Mobile Platform,” in 2010 International Conference on Computational Intelligence and Communication Networks, 2010, pp. 560-564.
\bibitem{3}
”Video and Image Processing Design Using FPGAs.” Altera. 2007. January 2014. 
http://www.altera.com/literature/wp/wp-video0306.pdf  
\bibitem{4}
E. Trucco and A. Verri, “Chapter 8 - Motion,” in Introductory Techniques for 3D Computer, pp. 177- 219.
\bibitem{5}
S.A. El-Azim et al., “An Efficient Object Tracking Technique Using Block-Matching Algorithm”, in Nineteenth National Radio Science Conference, Alexandria, 2002, pp. 427 - 433.
\bibitem{6} 
Caner et al., “An Adaptive Filtering Framework For Image Registration”, IEEE Trans. Acoustics, Speech, and Signal Processing, vol. 2, no. 2, 885-888. March, 2005. 
\bibitem{7}
Yin et al. \textit{Performance Evaluation of Object Tracking Algorithm} [Online]. Available: http://dircweb.kingston.ac.uk/ 
\bibitem{8}
G. Shrikanth, K. Subramanian, “Implementation of FPGA-Based object tracking algorithm,” Electronics and Communication Engineering Sri Venkateswara College of Engineering, 2008.
\bibitem{9}
E. Pizzini, D. Thomas, “FPGA Based Kalman Filter,” Worcester Polytechnic Institute, 2012.
\bibitem{10}
M. Shabany. (2011, December 27). \textit{Floating-point to Fixed-point Conversion} [Online]. Available: http://ee.sharif.edu/~digitalvlsi/Docs/Fixed-Point.pdf
\bibitem{11} 
N. Devillard. (1998, July). \textit{Fast median search: an ANSI C implementation} [Online]. Available: http://ndevilla.free.fr/median/median.pdf
\bibitem{12}
D. Kohanbash. (2014, January 30). \textit{Kalman Filtering - A Practical Implementation Guide (with code!)} [Online]. Available: http://robotsforroboticists.com/kalman-filtering/
\end{thebibliography}
\newpage
\appendix
\appendixpage
\section{Additional Figures} \label{sec:add}
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{./images/DE1_schem.jpg}
\caption{Altera DE1 Schematic (Partial)}
\label{fig:de1}    
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{./images/DE2_schem.jpg}
\caption{Altera DE2 Schematic (Partial)}
\label{fig:de2}    
\end{figure}
\end{document}